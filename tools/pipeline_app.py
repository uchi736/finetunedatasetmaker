"""
LLMå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ GUI

èµ·å‹•: streamlit run tools/pipeline_app.py
"""

import streamlit as st
import subprocess
import json
import os
import sys
from pathlib import Path

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="Dataset Pipeline",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem 2rem;
        border-radius: 12px;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
    }
    .main-header h1 {
        color: white;
        margin: 0;
        font-size: 1.8rem;
        font-weight: 600;
    }
    .main-header p {
        color: rgba(255,255,255,0.85);
        margin: 0.3rem 0 0 0;
        font-size: 0.95rem;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        padding: 10px 20px;
        border-radius: 8px 8px 0 0;
    }
    .command-box {
        background: #1e1e2e;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .section-header {
        display: flex;
        align-items: center;
        gap: 8px;
        margin-bottom: 0.5rem;
    }
    div[data-testid="stExpander"] {
        border: 1px solid #e0e0e0;
        border-radius: 10px;
        margin-bottom: 0.8rem;
    }
</style>
""", unsafe_allow_html=True)

# ãƒ˜ãƒƒãƒ€ãƒ¼
st.markdown("""
<div class="main-header">
    <h1>ğŸ“Š LLMå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³</h1>
    <p>PDF â†’ JSONL å¤‰æ›ãƒ»æ‹¡å¼µãƒ»ãƒ‘ãƒƒã‚­ãƒ³ã‚°</p>
</div>
""", unsafe_allow_html=True)

# ã‚¿ãƒ–æ§‹æˆ
tab1, tab2, tab3 = st.tabs(["ğŸ“¦ ãƒãƒƒãƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³", "âš™ï¸ å€‹åˆ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ", "ğŸ‘ï¸ ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼"])

# =============================================================================
# Tab 1: ãƒãƒƒãƒãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# =============================================================================
with tab1:
    # PDFãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’å–å¾—
    input_dir_default = "data/input"
    try:
        pdf_count = len(list(Path(input_dir_default).glob("*.pdf")))
    except:
        pdf_count = 0

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚«ãƒ¼ãƒ‰
    m1, m2, m3, m4 = st.columns(4)
    with m1:
        st.metric("ğŸ“ å…¥åŠ›PDF", f"{pdf_count}ä»¶")
    with m2:
        st.metric("ğŸ“„ å‡ºåŠ›å½¢å¼", "JSONL")
    with m3:
        st.metric("ğŸ”§ ãƒ¢ãƒ¼ãƒ‰", "ãƒãƒƒãƒå‡¦ç†")
    with m4:
        st.metric("ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", "æº–å‚™å®Œäº†")

    st.divider()

    # å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰é¸æŠï¼ˆãƒ•ã‚©ãƒ¼ãƒ å¤– - ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿æ™‚ã®ã¿å†æç”»ï¼‰
    input_mode = st.radio(
        "å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰",
        ["ğŸ“ PDFãƒ•ã‚©ãƒ«ãƒ€", "ğŸ“„ JSONLãƒ•ã‚¡ã‚¤ãƒ«"],
        horizontal=True,
        help="PDFã‹ã‚‰æ–°è¦ä½œæˆ or æ—¢å­˜JSONLã‚’æ‹¡å¼µ",
        key="input_mode_radio"
    )

    # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’äº‹å‰å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ å¤–ã§å®Ÿè¡Œï¼‰
    jsonl_search_dirs = ["data/output", "data", "."]
    jsonl_files = []
    for search_dir in jsonl_search_dirs:
        if Path(search_dir).exists():
            jsonl_files.extend(Path(search_dir).glob("*.jsonl"))
            jsonl_files.extend(Path(search_dir).glob("**/*.jsonl"))
    jsonl_files = sorted(set(str(f) for f in jsonl_files))

    # ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ å†…å¤–ã§ä½¿ç”¨ï¼‰
    is_pdf_mode = "PDF" in input_mode

    # è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ãªã— - æ¡ä»¶åˆ†å²ã®å•é¡Œã‚’å›é¿ï¼‰
    # å…¥å‡ºåŠ›è¨­å®š
    with st.expander("ğŸ“‚ å…¥å‡ºåŠ›è¨­å®š", expanded=True):
        col1, col2 = st.columns(2)
        with col1:
            # PDFãƒ¢ãƒ¼ãƒ‰ç”¨å…¥åŠ›
            input_path_pdf = st.text_input(
                "å…¥åŠ›ãƒ•ã‚©ãƒ«ãƒ€ (PDF)",
                value=input_dir_default,
                key="batch_input",
                help="PDFãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€",
                disabled=not is_pdf_mode
            )
            # JSONLãƒ¢ãƒ¼ãƒ‰ç”¨å…¥åŠ›
            if jsonl_files:
                input_path_jsonl = st.selectbox(
                    "å…¥åŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«",
                    options=[""] + jsonl_files,
                    key="batch_input_jsonl",
                    help="æ‹¡å¼µã—ãŸã„JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
                    disabled=is_pdf_mode
                )
            else:
                input_path_jsonl = st.text_input(
                    "å…¥åŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«",
                    value="data/output/preprocessed.jsonl",
                    key="batch_input_jsonl_manual",
                    help="æ‹¡å¼µã—ãŸã„JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹",
                    disabled=is_pdf_mode
                )
            # å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ãƒ‘ã‚¹ã‚’æ±ºå®š
            input_path = input_path_pdf if is_pdf_mode else input_path_jsonl
        with col2:
            output_file = st.text_input(
                "å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«",
                value="data/output/train.jsonl",
                key="batch_output",
                help="ç”Ÿæˆã•ã‚Œã‚‹JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
            )

    # PDFå‡¦ç†è¨­å®šï¼ˆå¸¸ã«è¡¨ç¤ºã€JSONLãƒ¢ãƒ¼ãƒ‰æ™‚ã¯ç„¡åŠ¹ï¼‰
    with st.expander("ğŸ”§ PDFå‡¦ç†è¨­å®š", expanded=False):
        if not is_pdf_mode:
            st.caption("ğŸ’¡ PDFãƒ¢ãƒ¼ãƒ‰é¸æŠæ™‚ã«æœ‰åŠ¹ã«ãªã‚Šã¾ã™")
        col1, col2, col3 = st.columns(3)
        with col1:
            chunk_size = st.number_input(
                "ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º",
                value=1500,
                min_value=100,
                max_value=10000,
                step=100,
                help="ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã™ã‚‹éš›ã®æœ€å¤§æ–‡å­—æ•°",
                disabled=not is_pdf_mode
            )
        with col2:
            chunk_overlap = st.number_input(
                "ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—",
                value=100,
                min_value=0,
                max_value=500,
                step=10,
                help="ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°",
                disabled=not is_pdf_mode
            )
        with col3:
            use_azure_di = st.checkbox(
                "Azure DI ä½¿ç”¨",
                value=False,
                help="Azure Document Intelligenceã§é«˜ç²¾åº¦æŠ½å‡º",
                disabled=not is_pdf_mode
            )

    # Azure DIè©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå¸¸ã«è¡¨ç¤ºï¼‰
    with st.expander("ğŸ” Azure DI è©³ç´°ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
        if not is_pdf_mode:
            st.caption("ğŸ’¡ PDFãƒ¢ãƒ¼ãƒ‰ + Azure DIä½¿ç”¨æ™‚ã«æœ‰åŠ¹ã«ãªã‚Šã¾ã™")
        elif not use_azure_di:
            st.caption("ğŸ’¡ Azure DIä½¿ç”¨ã«ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã‚‹ã¨æœ‰åŠ¹ã«ãªã‚Šã¾ã™")
        di_col1, di_col2, di_col3 = st.columns(3)
        with di_col1:
            extract_figures = st.checkbox(
                "å›³ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–",
                value=True,
                help="Vision API ã§å›³ã‚’èª¬æ˜æ–‡ã«å¤‰æ›",
                disabled=not (is_pdf_mode and use_azure_di)
            )
        with di_col2:
            convert_tables = st.checkbox(
                "è¡¨ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–",
                value=True,
                help="HTMLè¡¨ã‚’LLMã§æ–‡ç« ã«å¤‰æ›",
                disabled=not (is_pdf_mode and use_azure_di)
            )
        with di_col3:
            save_markdown = st.checkbox(
                "Markdownã‚’ä¿å­˜",
                value=True,
                help="output/markdown/ ã«ä¿å­˜",
                disabled=not (is_pdf_mode and use_azure_di)
            )

    # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    with st.expander("ğŸ§¹ ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°", expanded=False):
        st.caption("PDFã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒã‚¤ã‚ºé™¤å»è¨­å®š")
        clean_level = st.select_slider(
            "ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ãƒ™ãƒ«",
            options=["off", "basic", "aggressive"],
            value="basic",
            format_func=lambda x: {"off": "ãªã—", "basic": "åŸºæœ¬", "aggressive": "ç©æ¥µçš„"}[x],
            help="off=ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãªã—, basic=ãƒšãƒ¼ã‚¸ç•ªå·ãƒ»ç›®æ¬¡é™¤å», aggressive=ãƒ˜ãƒƒãƒ€/ãƒ•ãƒƒã‚¿è‡ªå‹•æ¤œå‡º"
        )

        if clean_level != "off":
            st.markdown("**é©ç”¨ã•ã‚Œã‚‹å‡¦ç†:**")
            checks = []
            if clean_level in ["basic", "aggressive"]:
                checks.extend([
                    "âœ… Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰",
                    "âœ… ãƒšãƒ¼ã‚¸ç•ªå·é™¤å»",
                    "âœ… ç›®æ¬¡é™¤å»",
                    "âœ… æ”¹è¡Œä¿®å¾©ï¼ˆå˜èªåˆ†æ–­ã®ä¿®æ­£ï¼‰",
                    "âœ… æ•°å€¤ãƒ»å˜ä½æ­£è¦åŒ–",
                ])
            if clean_level == "aggressive":
                checks.extend([
                    "âœ… ãƒ˜ãƒƒãƒ€/ãƒ•ãƒƒã‚¿è‡ªå‹•æ¤œå‡ºãƒ»é™¤å»",
                    "âœ… æ–­ç‰‡æ–‡ãƒ•ã‚£ãƒ«ã‚¿",
                ])
            st.markdown("  \n".join(checks))

    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µè¨­å®š
    with st.expander("âœ¨ ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µè¨­å®š", expanded=False):
        augment = st.toggle("ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’æœ‰åŠ¹åŒ–", value=True)

        if augment:
            # è¾æ›¸/ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹ï¼ˆLLMä¸è¦ï¼‰
            st.markdown("**è¾æ›¸/ã‚°ãƒ©ãƒ•ãƒ™ãƒ¼ã‚¹** (LLMä¸è¦)")
            col1, col2, col3 = st.columns(3)
            with col1:
                aug_dictionary = st.checkbox("ğŸ“š è¾æ›¸å®šç¾©", value=False)
            with col2:
                aug_generalized = st.checkbox("ğŸ”„ ä¸€èˆ¬åŒ–", value=False)
            with col3:
                aug_graph = st.checkbox("ğŸ”— ã‚°ãƒ©ãƒ•é–¢ä¿‚æ€§", value=False, help="â€»LLMä½¿ç”¨")

            # LLMãƒ™ãƒ¼ã‚¹
            st.markdown("**LLMãƒ™ãƒ¼ã‚¹**")
            col1, col2, col3 = st.columns(3)
            with col1:
                aug_paraphrase = st.checkbox("ğŸ’¬ è¨€ã„æ›ãˆ", value=True)
            with col2:
                aug_qa = st.checkbox("â“ Q&A", value=True)
            with col3:
                aug_summary = st.checkbox("ğŸ“ è¦ç´„", value=False)

            col1, col2 = st.columns(2)
            with col1:
                aug_keywords = st.checkbox("ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value=False)
            with col2:
                aug_discussion = st.checkbox("ğŸ’­ è­°è«–å½¢å¼", value=False)

            # LLMãƒ™ãƒ¼ã‚¹ä½¿ç”¨æ™‚ã®æ³¨æ„
            if aug_paraphrase or aug_qa or aug_summary or aug_keywords or aug_discussion or aug_graph:
                st.caption("âš ï¸ LLMãƒ™ãƒ¼ã‚¹æ‹¡å¼µã«ã¯ `AZURE_OPENAI_ENDPOINT` ã¨ `AZURE_OPENAI_API_KEY` ãŒå¿…è¦ã§ã™")

            # ç¿»è¨³
            st.markdown("**ç¿»è¨³**")
            col1, col2 = st.columns(2)
            with col1:
                aug_en = st.checkbox("ğŸ‡ºğŸ‡¸ è‹±èª", value=False)
            with col2:
                aug_zh = st.checkbox("ğŸ‡¨ğŸ‡³ ä¸­å›½èª", value=False)

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
            if aug_dictionary or aug_generalized or aug_graph:
                st.markdown("**ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹**")
                col1, col2 = st.columns(2)
                with col1:
                    dict_file = st.text_input("è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«", value="data/dict/terms.json", key="dict_file")
                with col2:
                    graph_file = st.text_input("ã‚°ãƒ©ãƒ•ãƒ•ã‚¡ã‚¤ãƒ«", value="data/graph/graph.json", key="graph_file")
            else:
                dict_file = "data/dict/terms.json"
                graph_file = "data/graph/graph.json"
        else:
            aug_paraphrase = aug_qa = aug_summary = False
            aug_keywords = aug_discussion = False
            aug_en = aug_zh = False
            aug_dictionary = aug_generalized = aug_graph = False
            dict_file = "data/dict/terms.json"
            graph_file = "data/graph/graph.json"

    # ãƒ‘ãƒƒã‚­ãƒ³ã‚°è¨­å®š
    with st.expander("ğŸ“¦ ãƒ‘ãƒƒã‚­ãƒ³ã‚°è¨­å®š", expanded=True):
        col1, col2 = st.columns([1, 2])
        with col1:
            pack = st.toggle("ãƒ‘ãƒƒã‚­ãƒ³ã‚°æœ‰åŠ¹åŒ–", value=True)
        with col2:
            if pack:
                max_seq_len = st.select_slider(
                    "æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·",
                    options=[1024, 2048, 4096, 8192],
                    value=2048,
                    help="ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ä¸Šé™"
                )
            else:
                max_seq_len = 2048
                st.caption("ãƒ‘ãƒƒã‚­ãƒ³ã‚°ç„¡åŠ¹æ™‚ã¯å„ãƒãƒ£ãƒ³ã‚¯ãŒãã®ã¾ã¾å‡ºåŠ›ã•ã‚Œã¾ã™")

    # ãã®ä»–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    with st.expander("âš¡ ãã®ä»–ã‚ªãƒ—ã‚·ãƒ§ãƒ³", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            no_shuffle = st.checkbox("ğŸ”€ ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„", value=False)
        with col2:
            keep_intermediate = st.checkbox("ğŸ’¾ ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ä¿æŒ", value=False)

        st.markdown("**ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—**")
        use_tokenizer = st.checkbox("ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§æ­£ç¢ºã«è¨ˆç®—", value=False, help="æŒ‡å®šã—ãªã„å ´åˆã¯æ¨å®šå€¤")
        if use_tokenizer:
            tokenizer_name = st.text_input(
                "ãƒ¢ãƒ‡ãƒ«å",
                value="llm-jp/llm-jp-3-13b",
                help="HuggingFaceã®ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: llm-jp/llm-jp-3-13b, meta-llama/Llama-2-7bï¼‰"
            )
        else:
            tokenizer_name = None

    # è¨­å®šç¢ºå®šãƒœã‚¿ãƒ³ï¼ˆé€šå¸¸ã®ãƒœã‚¿ãƒ³ï¼‰
    submitted = st.button("âš™ï¸ è¨­å®šã‚’ç¢ºå®š", type="secondary", use_container_width=True)

    st.divider()

    # ã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆï¼ˆè¨­å®šç¢ºå®šå¾Œã®ã¿è¡¨ç¤ºï¼‰
    # session_stateã§è¨­å®šã‚’ä¿æŒ
    if submitted:
        st.session_state["settings_confirmed"] = True
        st.session_state["cfg"] = {
            "input_mode": input_mode,
            "input_path": input_path,
            "output_file": output_file,
            "chunk_size": chunk_size if "PDF" in input_mode else 1500,
            "chunk_overlap": chunk_overlap if "PDF" in input_mode else 100,
            "use_azure_di": use_azure_di if "PDF" in input_mode else False,
            "extract_figures": extract_figures if "PDF" in input_mode and use_azure_di else False,
            "convert_tables": convert_tables if "PDF" in input_mode and use_azure_di else False,
            "save_markdown": save_markdown if "PDF" in input_mode and use_azure_di else False,
            "clean_level": clean_level,
            "augment": augment,
            "aug_paraphrase": aug_paraphrase if augment else False,
            "aug_qa": aug_qa if augment else False,
            "aug_summary": aug_summary if augment else False,
            "aug_keywords": aug_keywords if augment else False,
            "aug_discussion": aug_discussion if augment else False,
            "aug_en": aug_en if augment else False,
            "aug_zh": aug_zh if augment else False,
            "aug_dictionary": aug_dictionary if augment else False,
            "aug_generalized": aug_generalized if augment else False,
            "aug_graph": aug_graph if augment else False,
            "dict_file": dict_file if augment else "data/dict/terms.json",
            "graph_file": graph_file if augment else "data/graph/graph.json",
            "pack": pack,
            "max_seq_len": max_seq_len,
            "no_shuffle": no_shuffle,
            "keep_intermediate": keep_intermediate,
            "tokenizer_name": tokenizer_name,
        }
        st.rerun()

    # è¨­å®šç¢ºå®šæ¸ˆã¿ã®å ´åˆã®ã¿ã‚³ãƒãƒ³ãƒ‰è¡¨ç¤º
    if not st.session_state.get("settings_confirmed"):
        st.info("ğŸ‘† è¨­å®šã‚’é¸æŠã—ã¦ã€Œè¨­å®šã‚’ç¢ºå®šã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")
        st.stop()

    # ä¿å­˜ã•ã‚ŒãŸè¨­å®šã‚’ä½¿ç”¨
    cfg = st.session_state.get("cfg", {})
    input_mode = cfg.get("input_mode", input_mode)
    input_path = cfg.get("input_path", input_path if "input_path" in dir() else "data/input")
    output_file = cfg.get("output_file", output_file if "output_file" in dir() else "data/output/train.jsonl")
    chunk_size = cfg.get("chunk_size", 1500)
    chunk_overlap = cfg.get("chunk_overlap", 100)
    use_azure_di = cfg.get("use_azure_di", False)
    extract_figures = cfg.get("extract_figures", False)
    convert_tables = cfg.get("convert_tables", False)
    save_markdown = cfg.get("save_markdown", False)
    clean_level = cfg.get("clean_level", "basic")
    augment = cfg.get("augment", True)
    aug_paraphrase = cfg.get("aug_paraphrase", True)
    aug_qa = cfg.get("aug_qa", True)
    aug_summary = cfg.get("aug_summary", False)
    aug_keywords = cfg.get("aug_keywords", False)
    aug_discussion = cfg.get("aug_discussion", False)
    aug_en = cfg.get("aug_en", False)
    aug_zh = cfg.get("aug_zh", False)
    aug_dictionary = cfg.get("aug_dictionary", False)
    aug_generalized = cfg.get("aug_generalized", False)
    aug_graph = cfg.get("aug_graph", False)
    dict_file = cfg.get("dict_file", "data/dict/terms.json")
    graph_file = cfg.get("graph_file", "data/graph/graph.json")
    pack = cfg.get("pack", True)
    max_seq_len = cfg.get("max_seq_len", 2048)
    no_shuffle = cfg.get("no_shuffle", False)
    keep_intermediate = cfg.get("keep_intermediate", False)
    tokenizer_name = cfg.get("tokenizer_name", None)

    # ã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆ
    cmd = [sys.executable, "scripts/batch_pipeline.py", input_path, "-o", output_file]
    # PDFãƒ¢ãƒ¼ãƒ‰ã®ã¿ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
    if "PDF" in input_mode:
        cmd += ["--chunk-size", str(chunk_size)]
        cmd += ["--chunk-overlap", str(chunk_overlap)]
    if use_azure_di:
        cmd.append("--use-azure-di")
        if extract_figures:
            cmd.append("--extract-figures")
        if convert_tables:
            cmd.append("--convert-tables")
        if save_markdown:
            cmd.append("--save-markdown")
    if clean_level != "basic":  # basicã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãªã®ã§çœç•¥
        cmd += ["--clean-level", clean_level]
    if augment:
        cmd.append("--augment")
        if aug_paraphrase:
            cmd.append("--aug-paraphrase")
        if aug_qa:
            cmd.append("--aug-qa")
        if aug_summary:
            cmd.append("--aug-summary")
        if aug_keywords:
            cmd.append("--aug-keywords")
        if aug_discussion:
            cmd.append("--aug-discussion")
        if aug_en:
            cmd.append("--aug-translation-en")
        if aug_zh:
            cmd.append("--aug-translation-zh")
        if aug_dictionary:
            cmd.append("--aug-dictionary")
        if aug_generalized:
            cmd.append("--aug-generalized")
        if aug_graph:
            cmd.append("--aug-graph")
        if dict_file != "data/dict/terms.json":
            cmd += ["--dict-file", dict_file]
        if graph_file != "data/graph/graph.json":
            cmd += ["--graph-file", graph_file]
    if pack:
        cmd.append("--pack")
        cmd += ["--max-seq-len", str(max_seq_len)]
    if no_shuffle:
        cmd.append("--no-shuffle")
    if keep_intermediate:
        cmd.append("--keep-intermediate")
    if tokenizer_name:
        cmd += ["--tokenizer", tokenizer_name]

    # ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    st.markdown("##### ğŸ–¥ï¸ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰")
    st.code(" ".join(cmd), language="bash")

    # å®Ÿè¡Œãƒ»ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
    col1, col2, col3 = st.columns([1, 1, 1])
    with col1:
        if st.button("ğŸ”„ è¨­å®šå¤‰æ›´", key="reset_settings", use_container_width=True):
            st.session_state["settings_confirmed"] = False
            st.rerun()
    with col2:
        run_button = st.button("ğŸš€ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ", key="run_batch", type="primary", use_container_width=True)

    if run_button:
        with st.status("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­...", expanded=True) as status:
            input_type = "PDF" if "PDF" in input_mode else "JSONL"
            st.write(f"ğŸ“ {input_type}ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­...")
            env = os.environ.copy()
            env["PYTHONIOENCODING"] = "utf-8"
            result = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", env=env)

            if result.returncode == 0:
                st.write(f"âœ… {input_type}å‡¦ç†å®Œäº†")
                st.write("âœ… ãƒ‘ãƒƒã‚­ãƒ³ã‚°å®Œäº†" if pack else "âœ… å¤‰æ›å®Œäº†")
                st.write("âœ… ãƒãƒ¼ã‚¸å®Œäº†")
                status.update(label="âœ¨ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!", state="complete", expanded=False)
                st.toast("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!", icon="âœ…")
            else:
                status.update(label="âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ", state="error")
                st.toast("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", icon="âŒ")

        # ãƒ­ã‚°è¡¨ç¤ºï¼ˆstatusãƒ–ãƒ­ãƒƒã‚¯ã®å¤–ï¼‰
        if result.returncode == 0:
            # çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡ºã—ã¦è¡¨ç¤º
            import re
            stdout = result.stdout or ""
            count_match = re.search(r'ç·ãƒ‡ãƒ¼ã‚¿æ•°: ([\d,]+)ä»¶', stdout)
            chars_match = re.search(r'åˆè¨ˆæ–‡å­—æ•°: ([\d,]+)æ–‡å­—', stdout)
            tokens_match = re.search(r'(?:æ¨å®š)?ãƒˆãƒ¼ã‚¯ãƒ³æ•°: ([\d,]+)ãƒˆãƒ¼ã‚¯ãƒ³', stdout)
            is_estimated = "æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°" in stdout

            if count_match and chars_match and tokens_match:
                st.markdown("##### ğŸ“Š ç”Ÿæˆãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ğŸ“„ ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", count_match.group(1) + "ä»¶")
                with col2:
                    st.metric("ğŸ“ åˆè¨ˆæ–‡å­—æ•°", chars_match.group(1) + "å­—")
                with col3:
                    token_label = "ğŸ”¢ æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³" if is_estimated else "ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
                    st.metric(token_label, tokens_match.group(1))

            with st.expander("ğŸ“‹ å®Ÿè¡Œãƒ­ã‚°", expanded=False):
                st.text(stdout)
        else:
            st.error("å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            st.text(result.stderr or result.stdout)

# =============================================================================
# Tab 2: å€‹åˆ¥ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# =============================================================================
with tab2:
    script_options = {
        "ğŸ“š è¾æ›¸ã‹ã‚‰ç”¨èªå®šç¾©": ("scripts/augment/expand_dictionary.py", "è¾æ›¸ãƒ™ãƒ¼ã‚¹", False),
        "ğŸ“– å°‚é–€ç”¨èªã«èª¬æ˜è¿½åŠ ": ("scripts/augment/expand_elaboration.py", "è¾æ›¸ãƒ™ãƒ¼ã‚¹", False),
        "ğŸ”„ å°‚é–€ç”¨èªâ†’ä¸€èˆ¬ç”¨èª": ("scripts/augment/expand_generalized.py", "è¾æ›¸ãƒ™ãƒ¼ã‚¹", False),
        "ğŸ·ï¸ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º": ("scripts/augment/expand_keywords.py", "è¾æ›¸ãƒ™ãƒ¼ã‚¹", False),
        "â“ Q&Aç”Ÿæˆ": ("scripts/augment/expand_qa_difficult.py", "LLMä½¿ç”¨", True),
        "ğŸŒ è‹±èªç¿»è¨³": ("scripts/augment/expand_to_english.py", "LLMä½¿ç”¨", True),
        "ğŸ”— ã‚°ãƒ©ãƒ•é–¢ä¿‚æ€§": ("scripts/augment/expand_graph_relations.py", "LLMä½¿ç”¨", True),
        "ğŸ“¦ ãƒ‘ãƒƒã‚­ãƒ³ã‚°": ("scripts/preprocess/pack_sequences.py", "ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†", False),
    }

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    m1, m2, m3 = st.columns(3)
    with m1:
        st.metric("ğŸ“œ åˆ©ç”¨å¯èƒ½ã‚¹ã‚¯ãƒªãƒ—ãƒˆ", f"{len(script_options)}å€‹")
    with m2:
        st.metric("ğŸ¤– LLMå¿…è¦", "3å€‹")
    with m3:
        st.metric("ğŸ“š è¾æ›¸ãƒ™ãƒ¼ã‚¹", "4å€‹")

    st.divider()

    selected_script = st.selectbox(
        "ã‚¹ã‚¯ãƒªãƒ—ãƒˆé¸æŠ",
        list(script_options.keys()),
        format_func=lambda x: x
    )
    script_path, script_type, needs_llm = script_options[selected_script]

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆæƒ…å ±
    if needs_llm:
        st.warning(f"âš ï¸ ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯LLM APIã‚’ä½¿ç”¨ã—ã¾ã™ï¼ˆ{script_type}ï¼‰")
    else:
        st.info(f"â„¹ï¸ {script_type}å‡¦ç†")

    st.divider()

    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆåˆ¥ã®å…¥åŠ›
    if "pack_sequences" in script_path:
        with st.expander("ğŸ“¦ ãƒ‘ãƒƒã‚­ãƒ³ã‚°è¨­å®š", expanded=True):
            col1, col2 = st.columns(2)
            with col1:
                pack_input = st.text_input("å…¥åŠ›JSONL", value="data/output/sample.jsonl", key="pack_input")
            with col2:
                pack_output = st.text_input("å‡ºåŠ›JSONL", value="data/output/packed.jsonl", key="pack_output")

            col1, col2 = st.columns(2)
            with col1:
                pack_max_seq = st.select_slider("æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·", options=[1024, 2048, 4096, 8192], value=2048, key="pack_seq")
            with col2:
                pack_shuffle = st.checkbox("ğŸ”€ ã‚·ãƒ£ãƒƒãƒ•ãƒ«", value=False, key="pack_shuf")

        cmd2 = [sys.executable, script_path, pack_input, "-o", pack_output, "--max-seq-len", str(pack_max_seq)]
        if pack_shuffle:
            cmd2.append("--shuffle")

    elif "graph_relations" in script_path:
        with st.expander("ğŸ”— ã‚°ãƒ©ãƒ•è¨­å®š", expanded=True):
            col1, col2 = st.columns(2)
            with col1:
                graph_input = st.text_input("ã‚°ãƒ©ãƒ•JSON", value="data/graph/graph.json", key="graph_input")
            with col2:
                graph_output = st.text_input("å‡ºåŠ›JSONL", value="data/output/graph_relations.jsonl", key="graph_output")

            graph_limit = st.number_input("å‡¦ç†ãƒãƒ¼ãƒ‰æ•° (0=å…¨ã¦)", value=0, min_value=0, key="graph_limit")

        cmd2 = [sys.executable, script_path]
        if graph_input != "data/graph/graph.json":
            cmd2 += ["--input", graph_input]
        if graph_output != "data/output/graph_relations.jsonl":
            cmd2 += ["--output", graph_output]
        if graph_limit > 0:
            cmd2 += ["--limit", str(graph_limit)]
    else:
        st.caption("ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œã•ã‚Œã¾ã™")
        cmd2 = [sys.executable, script_path]

    # ã‚³ãƒãƒ³ãƒ‰ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    st.markdown("##### ğŸ–¥ï¸ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰")
    st.code(" ".join(cmd2), language="bash")

    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        run_script = st.button("ğŸš€ ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ", key="run_script", type="primary", use_container_width=True)

    if run_script:
        with st.status("ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¸­...", expanded=True) as status:
            st.write(f"â–¶ï¸ {selected_script} ã‚’å®Ÿè¡Œä¸­...")
            env = os.environ.copy()
            env["PYTHONIOENCODING"] = "utf-8"
            result = subprocess.run(cmd2, capture_output=True, text=True, encoding="utf-8", env=env)

            if result.returncode == 0:
                status.update(label="âœ¨ å®Ÿè¡Œå®Œäº†!", state="complete", expanded=False)
                st.toast("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!", icon="âœ…")
            else:
                status.update(label="âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ", state="error")
                st.toast("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", icon="âŒ")

        # ãƒ­ã‚°è¡¨ç¤ºï¼ˆstatusãƒ–ãƒ­ãƒƒã‚¯ã®å¤–ï¼‰
        if result.returncode == 0:
            with st.expander("ğŸ“‹ å®Ÿè¡Œãƒ­ã‚°", expanded=True):
                st.text(result.stdout)
        else:
            st.error("å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            st.text(result.stderr or result.stdout)

# =============================================================================
# Tab 3: ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼
# =============================================================================
with tab3:
    uploaded_file = st.file_uploader(
        "JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—ã¾ãŸã¯é¸æŠ",
        type=["jsonl", "json"],
        help="ç”Ÿæˆã•ã‚ŒãŸJSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…å®¹ã‚’ç¢ºèª"
    )

    if uploaded_file is not None:
        import pandas as pd

        records = []
        for line_num, line in enumerate(uploaded_file, 1):
            try:
                record = json.loads(line)
                record["_line"] = line_num
                records.append(record)
            except json.JSONDecodeError:
                pass

        if records:
            df = pd.DataFrame(records)
            cols = ["_line"] + [c for c in df.columns if c != "_line"]
            df = df[cols]

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            m1, m2, m3, m4 = st.columns(4)
            with m1:
                st.metric("ğŸ“Š ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°", len(records))
            with m2:
                if "augmentation_type" in df.columns:
                    st.metric("ğŸ·ï¸ æ‹¡å¼µã‚¿ã‚¤ãƒ—", df["augmentation_type"].nunique())
                else:
                    st.metric("ğŸ·ï¸ æ‹¡å¼µã‚¿ã‚¤ãƒ—", "-")
            with m3:
                avg_len = df["text"].str.len().mean() if "text" in df.columns else 0
                st.metric("ğŸ“ å¹³å‡æ–‡å­—æ•°", f"{avg_len:.0f}")
            with m4:
                st.metric("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å", uploaded_file.name[:15] + "..." if len(uploaded_file.name) > 15 else uploaded_file.name)

            st.divider()

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            col1, col2 = st.columns(2)
            with col1:
                if "augmentation_type" in df.columns:
                    types = ["å…¨ã¦"] + list(df["augmentation_type"].dropna().unique())
                    selected_type = st.selectbox("ğŸ·ï¸ ã‚¿ã‚¤ãƒ—ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼", types)
                    if selected_type != "å…¨ã¦":
                        df = df[df["augmentation_type"] == selected_type]
            with col2:
                search_text = st.text_input("ğŸ” ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢", placeholder="ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›...")
                if search_text:
                    mask = df.apply(lambda row: search_text.lower() in str(row).lower(), axis=1)
                    df = df[mask]

            # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµæœ
            st.caption(f"è¡¨ç¤º: **{len(df)}ä»¶** / {len(records)}ä»¶ä¸­")

            # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            st.dataframe(
                df.head(100),
                use_container_width=True,
                height=350,
                column_config={
                    "_line": st.column_config.NumberColumn("è¡Œ", width="small"),
                    "text": st.column_config.TextColumn("ãƒ†ã‚­ã‚¹ãƒˆ", width="large"),
                }
            )

            st.divider()

            # è©³ç´°è¡¨ç¤º
            st.markdown("##### ğŸ” ãƒ¬ã‚³ãƒ¼ãƒ‰è©³ç´°")
            col1, col2 = st.columns([1, 4])
            with col1:
                selected_line = st.number_input(
                    "è¡Œç•ªå·",
                    min_value=1,
                    max_value=len(records),
                    value=1,
                    key="detail_line"
                )
            with col2:
                st.caption(f"è¡Œ {selected_line} / {len(records)}")

            selected_record = records[selected_line - 1]

            if "text" in selected_record:
                st.text_area(
                    "ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹",
                    selected_record["text"],
                    height=200,
                    key="detail_text"
                )

            other_fields = {k: v for k, v in selected_record.items() if k not in ["text", "_line"]}
            if other_fields:
                with st.expander("ğŸ“‹ ãã®ä»–ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰", expanded=True):
                    st.json(other_fields)
        else:
            st.warning("æœ‰åŠ¹ãªJSONãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    else:
        # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
        st.markdown("""
        <div style="
            border: 2px dashed #ccc;
            border-radius: 12px;
            padding: 3rem;
            text-align: center;
            color: #888;
            margin: 2rem 0;
        ">
            <p style="font-size: 3rem; margin: 0;">ğŸ“„</p>
            <p style="font-size: 1.1rem; margin: 0.5rem 0;">JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—</p>
            <p style="font-size: 0.9rem; color: #aaa;">ã¾ãŸã¯ã€ŒBrowse filesã€ã‚’ã‚¯ãƒªãƒƒã‚¯</p>
        </div>
        """, unsafe_allow_html=True)

        with st.expander("ğŸ“– å¯¾å¿œå½¢å¼"):
            st.code('{"text": "...", "augmentation_type": "...", ...}', language="json")
